{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit scoring with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to use Deep Learning algorithms to resolve credit scoring problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Australia credit (http://archive.ics.uci.edu/ml/datasets/Statlog+(Australian+Credit+Approval)) <br>\n",
    "Descriptions: 15 attributes, 690 instances. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library\n",
    "There are severval frameworks used for Deep Learning: Theano, Tensorflow, PyTorch, Keras...\n",
    "\n",
    "In our tutorial, we will use Tensorflow and Keras.\n",
    "\n",
    "#### Tensorflow\n",
    "TensorFlow was developed by researchers and engineers from the Google Brain team. It is far and away the most commonly used software library in the field of deep learning.\n",
    "\n",
    "Install tensorflow: \n",
    "- For CPU only: pip install tensorflow\n",
    "- For CUDA-enabled GPU cards:pip install tensorflow-gpu\n",
    "\n",
    "#### Keras\n",
    "Keras is written in Python and can run on top of TensorFlow (as well as CNTK and Theano). \n",
    "\n",
    "Install Keras: pip install keras\n",
    "\n",
    "You can refer to the official Keras documentation (keras.io) to get a detailed understanding of how the framework works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df=pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat\",delim_whitespace=True, header=None)\n",
    "df=dataframe.fillna(dataframe.mean())\n",
    "dataset=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>11.460</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>11.500</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>8.170</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1</td>\n",
       "      <td>31.57</td>\n",
       "      <td>10.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.415</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>18.83</td>\n",
       "      <td>9.540</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>27.42</td>\n",
       "      <td>14.500</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3.085</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1       2   3   4   5      6   7   8   9   10  11   12    13  14\n",
       "0     1  22.08  11.460   2   4   4  1.585   0   0   0   1   2  100  1213   0\n",
       "1     0  22.67   7.000   2   8   4  0.165   0   0   0   0   2  160     1   0\n",
       "2     0  29.58   1.750   1   4   4  1.250   0   0   0   1   2  280     1   0\n",
       "3     0  21.67  11.500   1   5   3  0.000   1   1  11   1   2    0     1   1\n",
       "4     1  20.17   8.170   2   6   4  1.960   1   1  14   0   2   60   159   1\n",
       "..   ..    ...     ...  ..  ..  ..    ...  ..  ..  ..  ..  ..  ...   ...  ..\n",
       "685   1  31.57  10.500   2  14   4  6.500   1   0   0   0   2    0     1   1\n",
       "686   1  20.67   0.415   2   8   4  0.125   0   0   0   0   2    0    45   0\n",
       "687   0  18.83   9.540   2   6   4  0.085   1   0   0   0   2  100     1   1\n",
       "688   0  27.42  14.500   2  14   8  3.085   1   1   1   0   2  120    12   1\n",
       "689   1  41.00   0.040   2  10   4  0.040   0   1   1   0   1  560     1   1\n",
       "\n",
       "[690 rows x 15 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:,14].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_feature=len(dataset[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into input X and output Y variable\n",
    "X=scaled_data[:,0:no_feature].astype(float)\n",
    "Y=scaled_data[:,no_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(len(X)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "y_train = Y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = Y[train_size:]\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, None, 24)          576       \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 24)                4704      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 5,305\n",
      "Trainable params: 5,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model=Sequential()\n",
    "model.add(Embedding(24,output_dim=24))\n",
    "model.add(LSTM(24))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "552/552 [==============================] - 5s 10ms/step - loss: 0.6886 - acc: 0.5580\n",
      "Epoch 2/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.6402 - acc: 0.6504\n",
      "Epoch 3/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.5623 - acc: 0.7264A: 0s - loss: 0.5637 - acc: 0.728\n",
      "Epoch 4/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4980 - acc: 0.7971\n",
      "Epoch 5/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4641 - acc: 0.8116\n",
      "Epoch 6/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4768 - acc: 0.7953\n",
      "Epoch 7/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4340 - acc: 0.8116\n",
      "Epoch 8/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4539 - acc: 0.8098\n",
      "Epoch 9/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4290 - acc: 0.8207\n",
      "Epoch 10/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4585 - acc: 0.8225\n",
      "Epoch 11/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4510 - acc: 0.8261\n",
      "Epoch 12/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4467 - acc: 0.8406\n",
      "Epoch 13/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4371 - acc: 0.8080\n",
      "Epoch 14/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4464 - acc: 0.8297\n",
      "Epoch 15/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4470 - acc: 0.8297\n",
      "Epoch 16/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4615 - acc: 0.8116\n",
      "Epoch 17/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4377 - acc: 0.8243\n",
      "Epoch 18/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4299 - acc: 0.8279\n",
      "Epoch 19/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4401 - acc: 0.8243\n",
      "Epoch 20/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4196 - acc: 0.8225A: 0s - loss: 0.4170 - acc\n",
      "Epoch 21/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4475 - acc: 0.8279\n",
      "Epoch 22/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4349 - acc: 0.8442\n",
      "Epoch 23/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4411 - acc: 0.8297\n",
      "Epoch 24/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4253 - acc: 0.8170\n",
      "Epoch 25/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4452 - acc: 0.8207\n",
      "Epoch 26/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4350 - acc: 0.8370\n",
      "Epoch 27/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4298 - acc: 0.8514\n",
      "Epoch 28/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4240 - acc: 0.8279\n",
      "Epoch 29/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4173 - acc: 0.8370A: 0s - loss: 0.4602 - acc: \n",
      "Epoch 30/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4208 - acc: 0.8370A: 0s - loss: 0.4694 - ac\n",
      "Epoch 31/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4182 - acc: 0.8351A: 0s - loss: 0.3833 - \n",
      "Epoch 32/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3962 - acc: 0.8406\n",
      "Epoch 33/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4224 - acc: 0.8406\n",
      "Epoch 34/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4307 - acc: 0.8243\n",
      "Epoch 35/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4150 - acc: 0.8442\n",
      "Epoch 36/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4184 - acc: 0.8388\n",
      "Epoch 37/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4239 - acc: 0.8351\n",
      "Epoch 38/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4188 - acc: 0.8333\n",
      "Epoch 39/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4185 - acc: 0.8279\n",
      "Epoch 40/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4153 - acc: 0.8333\n",
      "Epoch 41/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4031 - acc: 0.8406\n",
      "Epoch 42/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4100 - acc: 0.8370\n",
      "Epoch 43/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4308 - acc: 0.8460\n",
      "Epoch 44/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4008 - acc: 0.8442\n",
      "Epoch 45/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3860 - acc: 0.8315\n",
      "Epoch 46/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4230 - acc: 0.8478\n",
      "Epoch 47/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4013 - acc: 0.8478\n",
      "Epoch 48/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3979 - acc: 0.8333\n",
      "Epoch 49/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4025 - acc: 0.8388\n",
      "Epoch 50/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3952 - acc: 0.8442\n",
      "Epoch 51/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4090 - acc: 0.8569\n",
      "Epoch 52/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3912 - acc: 0.8315\n",
      "Epoch 53/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3844 - acc: 0.8333\n",
      "Epoch 54/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3882 - acc: 0.8460\n",
      "Epoch 55/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4084 - acc: 0.8478\n",
      "Epoch 56/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3962 - acc: 0.8496A: 0s - loss: 0.3898 - acc: 0.85\n",
      "Epoch 57/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4046 - acc: 0.8297\n",
      "Epoch 58/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3954 - acc: 0.8605\n",
      "Epoch 59/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4097 - acc: 0.8442\n",
      "Epoch 60/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3789 - acc: 0.8442\n",
      "Epoch 61/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3613 - acc: 0.8533\n",
      "Epoch 62/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4025 - acc: 0.8424\n",
      "Epoch 63/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4049 - acc: 0.8406\n",
      "Epoch 64/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3851 - acc: 0.8533\n",
      "Epoch 65/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3869 - acc: 0.8533\n",
      "Epoch 66/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3753 - acc: 0.8533\n",
      "Epoch 67/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4073 - acc: 0.8478\n",
      "Epoch 68/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4186 - acc: 0.8442\n",
      "Epoch 69/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3770 - acc: 0.8496\n",
      "Epoch 70/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3749 - acc: 0.8514A: 0s - loss: 0.4003 - \n",
      "Epoch 71/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3933 - acc: 0.8514\n",
      "Epoch 72/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3833 - acc: 0.8388\n",
      "Epoch 73/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3773 - acc: 0.8533\n",
      "Epoch 74/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3763 - acc: 0.8424\n",
      "Epoch 75/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4060 - acc: 0.8388\n",
      "Epoch 76/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3819 - acc: 0.8478\n",
      "Epoch 77/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3971 - acc: 0.8370\n",
      "Epoch 78/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3789 - acc: 0.8442\n",
      "Epoch 79/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3752 - acc: 0.8533\n",
      "Epoch 80/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3912 - acc: 0.8514\n",
      "Epoch 81/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3756 - acc: 0.8623\n",
      "Epoch 82/100\n",
      "552/552 [==============================] - 1s 2ms/step - loss: 0.3686 - acc: 0.8351\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3712 - acc: 0.8478\n",
      "Epoch 84/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3903 - acc: 0.8569\n",
      "Epoch 85/100\n",
      "552/552 [==============================] - 1s 2ms/step - loss: 0.3833 - acc: 0.8551\n",
      "Epoch 86/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3930 - acc: 0.8370\n",
      "Epoch 87/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3864 - acc: 0.8533\n",
      "Epoch 88/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3721 - acc: 0.8533\n",
      "Epoch 89/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3802 - acc: 0.8569\n",
      "Epoch 90/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4023 - acc: 0.8388\n",
      "Epoch 91/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3960 - acc: 0.8514\n",
      "Epoch 92/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3876 - acc: 0.8442\n",
      "Epoch 93/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3820 - acc: 0.8569\n",
      "Epoch 94/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3740 - acc: 0.8460\n",
      "Epoch 95/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.4007 - acc: 0.8587\n",
      "Epoch 96/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3716 - acc: 0.8496A: 0s - loss: 0.3698 - acc: 0.8\n",
      "Epoch 97/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3721 - acc: 0.8551\n",
      "Epoch 98/100\n",
      "552/552 [==============================] - 1s 2ms/step - loss: 0.3881 - acc: 0.8496\n",
      "Epoch 99/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3755 - acc: 0.8496\n",
      "Epoch 100/100\n",
      "552/552 [==============================] - 1s 1ms/step - loss: 0.3784 - acc: 0.8478\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,  batch_size=10,  epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 5s 35ms/step\n",
      "Test accuracy: 0.8623188138008118\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, y_test, batch_size=1)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
